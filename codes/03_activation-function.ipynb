{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Sigmoid, Tanh, Softmax, ReLU\n",
    "from torch.nn.functional import leaky_relu, relu, softmax, tanh, sigmoid\n",
    "\n",
    "x = np.linspace(-10, +10, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "# useful in regression [last layer]. the output must be a continuous value.\n",
    "linear = x\n",
    "\n",
    "# step\n",
    "# disadvantage: same error value for all wrong samples\n",
    "step = np.where(x >= 0, 1, 0)\n",
    "\n",
    "# sign\n",
    "# disadvantage: same error value for all wrong samples\n",
    "sign = np.where(x < 0, -1, +1)\n",
    "\n",
    "# sigmoid\n",
    "# typically used in the last layer of binary classification\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# tanh : typically used in hidden layers\n",
    "# typically used in the middle layers\n",
    "tanh = 2 * (1 / (1 + np.exp(-2*x))) - 1 # 2 * sigmoid(2x) - 1\n",
    "tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "# relu\n",
    "# typically used in the middle layers\n",
    "# Use it to prevent gradient vanishing and exploding\n",
    "# x<0 -> 0 gradient -> dead neurons -> no update\n",
    "# it is not differentiable at exactly 0 [a subgradient of 0 is used at 0]\n",
    "relu = np.where(x > 0, x, 0)\n",
    "\n",
    "# leaky relu\n",
    "# improved version of ReLU [gradient leaks for x < 0]\n",
    "# it is not differentiable at exactly 0 [a subgradient of 0 is used at 0]\n",
    "leacky_relu = np.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "# softmax\n",
    "# typically used in the last layer of multi-class classification\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(nrows= 2, ncols= 4, figsize= (16, 8), layout= 'compressed')\n",
    "fig.suptitle(\"Activation Functions\")\n",
    "\n",
    "axs[0, 0].plot(x, linear)\n",
    "axs[0, 0].grid(True)\n",
    "axs[0, 0].set(title= 'linear')\n",
    "\n",
    "axs[0, 1].plot(x, step)\n",
    "axs[0, 1].grid(True)\n",
    "axs[0, 1].set(title= 'step', xticks= [0], yticks= [-1, 0, +1])\n",
    "\n",
    "axs[0, 2].plot(x, sign)\n",
    "axs[0, 2].grid(True)\n",
    "axs[0, 2].set(title= 'sign', xticks= [0], yticks= [-1, 0, +1])\n",
    "\n",
    "axs[0, 3].plot(x, sigmoid)\n",
    "axs[0, 3].grid(True)\n",
    "axs[0, 3].set(title= 'sigmoid', xticks= np.arange(-10, +12, 2), yticks= np.arange(0, 1.1, .1))\n",
    "\n",
    "axs[1, 0].plot(x, tanh)\n",
    "axs[1, 0].grid(True)\n",
    "axs[1, 0].set(title= 'TanH', xticks= np.arange(-10, +12, 2), yticks= np.arange(-1, 1.1, .2))\n",
    "\n",
    "axs[1, 1].plot(x, relu)\n",
    "axs[1, 1].grid(True)\n",
    "axs[1, 1].set(title= 'ReLU')\n",
    "\n",
    "axs[1, 2].plot(x, leacky_relu)\n",
    "axs[1, 2].grid(True)\n",
    "axs[1, 2].set(title= 'Leaky ReLU')\n",
    "\n",
    "axs[1, 3].plot(x, softmax(x))\n",
    "axs[1, 3].grid(True)\n",
    "axs[1, 3].set(title= 'softmax')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27caa3c2410>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_softmax(x):\n",
    "    exp_values = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    probabilities = exp_values / torch.sum(exp_values, dim=1, keepdim=True)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        exp_values = torch.exp(x - torch.max(x, dim= 1, keepdim= True)[0])\n",
    "        probabilities = exp_values / torch.sum(exp_values, dim= 1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel1(\n",
       "  (linear): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (activtion): CustomActivation()\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomModel1(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CustomModel1, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(3, 3)\n",
    "        self.activtion = CustomActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.activtion(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel1()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel2(\n",
       "  (linear): Linear(in_features=3, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomModel2(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CustomModel2, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = CustomActivation()(x)\n",
    "        return x\n",
    "\n",
    "model = CustomModel2()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "tensor([[5.6504e-01, 5.8902e-01, 1.6239e-01],\n",
      "        [1.1976e-03, 6.9004e-03, 9.2579e-01],\n",
      "        [5.6504e-01, 7.9282e-02, 1.4385e-04]], grad_fn=<DivBackward0>)\n",
      "\n",
      "predictions.argmax(dim= 1): tensor([1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.tensor([[1, 2, 3], [-4, -5, -6], [7, 8, 9]], dtype= torch.float32)\n",
    "train_y = torch.tensor([1, 2, 0], dtype= torch.float32)\n",
    "\n",
    "predictions = model(train_x)\n",
    "\n",
    "# log\n",
    "print(f\"predictions:\\n{predictions}\\n\")\n",
    "print(f\"predictions.argmax(dim= 1): {predictions.argmax(dim= 1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly applied to a tensor\n",
    "from torch import sign, sigmoid, tanh, relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the activation as a layer in a neural network\n",
    "from torch.nn import Sigmoid, Tanh, ReLU, LeakyReLU, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional form of the activations for use within larger functions\n",
    "from torch.nn.functional import sigmoid, tanh, relu, leaky_relu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain [-10, ..., +10]\n",
    "x = torch.linspace(-10, +10, 1001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "linear = x\n",
    "\n",
    "# sigmoid [torch.sigmoid()]\n",
    "sigmoid = 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# tanh [torch.tanh()]: 2 * sigmoid(2x) - 1\n",
    "tanh_1 = 2 * (1 / (1 + torch.exp(-2*x))) - 1\n",
    "tanh_2 = (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
    "\n",
    "# relu [torch.relu()]\n",
    "relu = torch.where(x > 0, x, 0)\n",
    "\n",
    "# leaky relu [torch.nn.functional.leaky_relu()]\n",
    "leacky_relu = torch.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "# softmax [torch.softmax()]\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows= 2, ncols= 3, figsize= (16, 12), layout= 'compressed')\n",
    "fig.suptitle(\"Activation Functions\")\n",
    "\n",
    "axs[0, 0].plot(x, linear)\n",
    "axs[0, 0].grid(True)\n",
    "axs[0, 0].set(title= 'linear')\n",
    "axs[0, 1].plot(x, sigmoid)\n",
    "axs[0, 1].grid(True)\n",
    "axs[0, 1].set(title= 'sigmoid', xticks= torch.arange(-10, +12, 2), yticks= torch.arange(0, 1.1, .1))\n",
    "axs[0, 2].plot(x, tanh_1)\n",
    "axs[0, 2].grid(True)\n",
    "axs[0, 2].set(title= 'TanH', xticks= torch.arange(-10, +12, 2), yticks= torch.arange(-1, 1.1, .2))\n",
    "axs[1, 0].plot(x, relu)\n",
    "axs[1, 0].grid(True)\n",
    "axs[1, 0].set(title= 'ReLU')\n",
    "axs[1, 1].plot(x, leacky_relu)\n",
    "axs[1, 1].grid(True)\n",
    "axs[1, 1].set(title= 'Leaky ReLU')\n",
    "axs[1, 2].plot(x, softmax(x))\n",
    "axs[1, 2].grid(True)\n",
    "axs[1, 2].set(title= 'softmax')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step\n",
    "step = torch.where(x >= 0, 1, 0)\n",
    "\n",
    "# sign [torch.sign()]\n",
    "sign = torch.where(x == 0, 0, torch.where(x < 0, -1, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows= 1, ncols= 2, figsize= (8, 4), layout= 'compressed')\n",
    "fig.suptitle(\"Threshold Functions\")\n",
    "\n",
    "axs[0].plot(x, step, color= 'red')\n",
    "axs[0].grid(True)\n",
    "axs[0].set(title= 'step', xticks= [0], yticks= [-1, 0, +1])\n",
    "axs[1].plot(x, sign, color= 'red')\n",
    "axs[1].grid(True)\n",
    "axs[1].set(title= 'sign', xticks= [0], yticks= [-1, 0, +1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

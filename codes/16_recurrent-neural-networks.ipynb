{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Dataset\n",
    "   - Regular datasets typically used in CNNs & MLPs are composed of independent data points\n",
    "   - Each data point is usually represented as a fixed-size vector (or tensor for images)\n",
    "   - **Notation**:\n",
    "      - $N$: Number of samples in the dataset.\n",
    "      - $\\mathbf{x}_i$: Input data point $i$, where $i \\in \\{1, 2, \\ldots, N\\}$.\n",
    "      - $\\mathbf{y}_i$: Label or target associated with input data $i$.\n",
    "   - **Formulation**:\n",
    "      - Dataset: $D=\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\mid i = 1, 2, \\ldots, N\\}$\n",
    "      - Each $\\mathbf{x}_i \\in \\R^M$, where $M$ is the dimensionality of the input feature vector\n",
    "   - **Example**: $D = \\{ (\\mathbf{x}_1, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{y}_2), (\\mathbf{x}_3, \\mathbf{y}_3) \\}$\n",
    "      - $\\mathbf{x}_1 = [1.0, 2.0], \\quad \\mathbf{y}_1 = 0$\n",
    "      - $\\mathbf{x}_2 = [2.5, 3.5], \\quad \\mathbf{y}_2 = 1$\n",
    "      - $\\mathbf{x}_3 = [0.5, 1.5], \\quad \\mathbf{y}_3 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[1.1000, 2.1000]]), label: tensor([0])\n",
      "data: tensor([[2.5000, 3.5000]]), label: tensor([1])\n",
      "data: tensor([[0.5000, 1.5000]]), label: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "class RegularDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.tensor([[1.1, 2.1], [2.5, 3.5], [0.5, 1.5]], dtype=torch.float32)\n",
    "        self.labels = torch.tensor([0, 1, 0], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# create dataset and dataloader\n",
    "dataset = RegularDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# iterate through the dataset\n",
    "for data, label in dataloader:\n",
    "    print(f\"data: {data}, label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Dataset\n",
    "   - Sequential datasets used in RNNs are composed of sequences of data points.\n",
    "   - Each sequence represents a temporal or sequential relationship among the data points\n",
    "   - **Notation**:\n",
    "      - $N$: Number of sequences in the dataset.\n",
    "      - $T$: Length of each sequence.\n",
    "      - $\\mathbf{x}^t_i$: Input data point at time step $t$ in the sequence $i$, where $t \\in \\{1, 2, \\ldots, T\\}$ and $i \\in \\{1, 2, \\ldots, N\\}$\n",
    "      - $\\mathbf{y}_i$: Label or target associated with sequence $i$.\n",
    "   - **Formulation**:\n",
    "      - Dataset: $D = \\{ (\\mathbf{x}_i^1, \\mathbf{x}_i^2, \\ldots, \\mathbf{x}_i^T, \\mathbf{y}_i) \\mid i = 1, 2, \\ldots, N \\}$\n",
    "      - Each $\\mathbf{x}^t_i \\in \\R^M$, where $M$ is the dimensionality of the input feature vector at each time step.\n",
    "   - **Example**: $D = \\{ (\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{y}_2), (\\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5, \\mathbf{y}_3) \\}$\n",
    "      - $\\mathbf{x}_1 = [1.0, 0.0]$\n",
    "      - $\\mathbf{x}_2 = [0.5, 1.5]$\n",
    "      - $\\mathbf{x}_3 = [1.0, 2.0]$\n",
    "      - $\\mathbf{x}_4 = [2.0, 1.0]$\n",
    "      - $\\mathbf{x}_5 = [1.5, 0.5]$\n",
    "      - $\\mathbf{y}_1 = 0$\n",
    "      - $\\mathbf{y}_2 = 1$\n",
    "      - $\\mathbf{y}_3 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:\n",
      "tensor([[[1.0000, 0.0000],\n",
      "         [0.5000, 1.5000],\n",
      "         [1.0000, 2.0000]]])\n",
      "label: tensor([0])\n",
      "\n",
      "sequence:\n",
      "tensor([[[2.0000, 1.0000],\n",
      "         [1.5000, 0.5000],\n",
      "         [2.5000, 1.5000]]])\n",
      "label: tensor([1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SequentialDatasetWithoutOverlap(Dataset):\n",
    "    def __init__(self):\n",
    "        # original data points\n",
    "        self.data = torch.tensor([\n",
    "            [1.0, 0.0],\n",
    "            [0.5, 1.5],\n",
    "            [1.0, 2.0],\n",
    "            [2.0, 1.0],\n",
    "            [1.5, 0.5],\n",
    "            [2.5, 1.5]\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # labels for each sequence\n",
    "        self.labels = torch.tensor([0, 1], dtype=torch.int64)\n",
    "\n",
    "        # sequence length\n",
    "        self.seq_length = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of sequences without overlap\n",
    "        return len(self.data) // self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # calculate the start index of the sequence\n",
    "        start_idx = idx * self.seq_length\n",
    "\n",
    "        # create a sequence of length seq_length\n",
    "        sequence = self.data[start_idx:start_idx + self.seq_length]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "# create dataset and dataloader\n",
    "dataset = SequentialDatasetWithoutOverlap()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# iterate through the dataset\n",
    "for sequence, label in dataloader:\n",
    "    print(f\"sequence:\\n{sequence}\\nlabel: {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence:\n",
      "tensor([[[0.5000, 1.5000],\n",
      "         [1.0000, 2.0000],\n",
      "         [2.0000, 1.0000]]])\n",
      "label: tensor([1])\n",
      "\n",
      "sequence:\n",
      "tensor([[[1.0000, 2.0000],\n",
      "         [2.0000, 1.0000],\n",
      "         [1.5000, 0.5000]]])\n",
      "label: tensor([0])\n",
      "\n",
      "sequence:\n",
      "tensor([[[1.0000, 0.0000],\n",
      "         [0.5000, 1.5000],\n",
      "         [1.0000, 2.0000]]])\n",
      "label: tensor([0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SequentialDatasetWithOverlap(Dataset):\n",
    "    def __init__(self):\n",
    "        # original data points\n",
    "        self.data = torch.tensor([\n",
    "            [1.0, 0.0],\n",
    "            [0.5, 1.5],\n",
    "            [1.0, 2.0],\n",
    "            [2.0, 1.0],\n",
    "            [1.5, 0.5]\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # labels for each sequence\n",
    "        self.labels = torch.tensor([0, 1, 0], dtype=torch.int64)\n",
    "\n",
    "        # sequence length\n",
    "        self.seq_length = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # create a sequence of length seq_length\n",
    "        sequence = self.data[idx:idx+self.seq_length]\n",
    "        label = self.labels[idx]\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "# create dataset and dataloader\n",
    "dataset = SequentialDatasetWithOverlap()\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# iterate through the dataset\n",
    "for sequence, label in dataloader:\n",
    "    print(f\"sequence:\\n{sequence}\\nlabel: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of sequence-to-sequence modeling configurations\n",
    "   - **One-to-One** (Single Input to Single Output):\n",
    "      - Simplest form of neural network where a single input is mapped to a single output\n",
    "      - Used in a standard feed-forward neural network (e.g. MLP or CNN based architectures)\n",
    "      - e.g. Image classification\n",
    "   - **One-to-Many** (Single Input to Sequence Output):\n",
    "      - A single input is processed by the RNN, which then produces a sequence of outputs over time.\n",
    "      - e.g. Image captioning (an image input resulting in a sequence of words).\n",
    "   - **Many-to-One** (Sequence Input to Single Output):\n",
    "      - The RNN processes each input in the sequence, and the final hidden state is used to produce the output\n",
    "      - e.g. Sentiment analysis (a sequence of words leading to a single sentiment label)\n",
    "   - **Many-to-Many** (Sequence Input to Sequence Output):\n",
    "      - A sequence of inputs leads to a sequence of outputs. This can be further divided into two subcategories:\n",
    "         - **Synchronized** Many-to-Many\n",
    "            - Each input in the sequence has a corresponding output\n",
    "            - The RNN processes a sequence of inputs, producing a corresponding output at each time step\n",
    "            - e.g. Video classification (each frame in a video results in a corresponding label)\n",
    "         - **Asynchronized** Many-to-Many\n",
    "            - The lengths of the input and output sequences can differ\n",
    "            - The RNN processes a sequence of inputs and generates a sequence of outputs which may have different lengths\n",
    "            - e.g. Machine translation (a sequence of words in one language translates to a sequence of words in another language)\n",
    "         \n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/seq-to-seq-modeling.svg\" alt=\"seq-to-seq-modeling.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">sequence-to-sequence modeling</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure: Recurrent Neural Networks\n",
    "   - RNNs are specifically designed to handle sequential data, where the order of elements matters\n",
    "   - Unlike feedforward neural networks, RNNs possess a \"memory\" component to process information from previous inputs, influencing the current output\n",
    "   - Each step in the sequence is processed by the same network (shared weights), with information passed between steps\n",
    "   - RNNs can suffer from vanishing and exploding gradients, making training difficult for long sequences.\n",
    "\n",
    "**RNN Variants**:\n",
    "   - Vanilla RNN\n",
    "   - Long Short-Term Memory (LSTM)\n",
    "      - Improves upon the vanilla RNN by introducing gates to control information flow\n",
    "   - Gated Recurrent Units (GRU)\n",
    "      - Simplifies the LSTM architecture while maintaining performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vanilla RNN\n",
    "   - **Notations**:\n",
    "      - $\\mathbf{x}_t$: input at time step $t$.\n",
    "      - $\\mathbf{h}_t$: Hidden state at time step $t$.\n",
    "      - $\\mathbf{y}_t$: Output at time step $t$.\n",
    "      - $\\mathbf{W}_{ih}$: Weight matrix for input to hidden\n",
    "      - $\\mathbf{W}_{hh}$: Weight matrix for hidden to hidden\n",
    "      - $\\mathbf{W}_{ho}$: Weight matrix for hidden to output\n",
    "      - $\\mathbf{b}_{ih}$: Bias for input to hidden\n",
    "      - $\\mathbf{b}_{hh}$: Bias for hidden to hidden\n",
    "      - $\\mathbf{b}_{ho}$: Bias for hidden to output\n",
    "      - $\\mathbf{\\sigma}$: Activation function (e.g., Tanh, Sigmoid, ReLU)\n",
    "      - $\\mathbf{g}$: Activation function for output (e.g., Softmax for classification)\n",
    "   - **Formulations**:\n",
    "      - Hidden State Calculation:\n",
    "      $$\\mathbf{h}_t = \\sigma(\\mathbf{W}_{ih} \\mathbf{x}_t + \\mathbf{b}_{ih} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_{hh}), \\quad \\mathbf{h}_0 = \\mathbf{0}$$\n",
    "      - Output Calculation:\n",
    "      $$\\mathbf{y}_t = g(\\mathbf{W}_{ho} \\mathbf{h}_t + \\mathbf{b}_{ho})$$\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/recurrent-neural-networks-1.svg\" alt=\"recurrent-neural-networks-1.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Folded Recurrent Neural Networks</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/recurrent-neural-networks-2.svg\" alt=\"recurrent-neural-networks-2.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Unfolded Recurrent Neural Networks</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # input to hidden connection weights\n",
    "        self.W_ih = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "        # input to hidden connection biases\n",
    "        self.b_ih = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "        # hidden to hidden connection weights\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        # hidden to hidden connection biases\n",
    "        self.b_hh = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "        # weights for hidden to output connection\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
    "        # bias for output layer\n",
    "        self.b_ho = nn.Parameter(torch.randn(output_dim))\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = torch.tanh(input @ self.W_ih.T + self.b_ih + hidden @ self.W_hh.T + self.b_hh)\n",
    "        output = hidden @ self.W_ho.T + self.b_ho\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self) -> torch.Tensor:\n",
    "        # initialize the hidden state with zeros (h_0)\n",
    "        return torch.zeros(self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 5\n",
    "\n",
    "# create RNN\n",
    "rnn_1 = VanillaRNN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()               : torch.Size([128, 5, 10])\n",
      "y.size()               : torch.Size([128])\n",
      "x.size() [first batch] : torch.Size([32, 5, 10])\n",
      "y.size() [first batch] : torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# example input (num data, sequence length, input dim)\n",
    "num_data = 128\n",
    "sequence_length = 5\n",
    "x = torch.randn(num_data, sequence_length, input_dim)\n",
    "y = torch.randn(num_data)\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(x, y)\n",
    "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# log\n",
    "print(f\"x.size()               : {x.size()}\")\n",
    "print(f\"y.size()               : {y.size()}\")\n",
    "print(f\"x.size() [first batch] : {next(iter(trainsetloader))[0].size()}\")\n",
    "print(f\"y.size() [first batch] : {next(iter(trainsetloader))[1].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# forward pass through the RNN\n",
    "for c, (x, y_true) in enumerate(trainsetloader):\n",
    "    # initialize hidden state\n",
    "    hidden = rnn_1.init_hidden()\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        y_pred, hidden = rnn_1(x[:, i, :], hidden)\n",
    "        print(f\"batch: {c+1}/{len(trainsetloader)} | time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VanillaRNN                               [32, 5]                   745\n",
       "==========================================================================================\n",
       "Total params: 745\n",
       "Trainable params: 745\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(rnn_1, input_size=((batch_size, input_dim), hidden.size()), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Weights and Concatenated Input and Hidden\n",
    "   - Reformulate the Vanilla RNN by:\n",
    "      - Combining the input-to-hidden and hidden-to-hidden weights into a single weight matrix\n",
    "      - Concatenating the input and hidden states together\n",
    "   - **Notations**:\n",
    "      - $\\mathbf{x}_t$: Input at time step $t$.\n",
    "      - $\\mathbf{h}_t$: Hidden state at time step $t$.\n",
    "      - $\\mathbf{y}_t$: Output at time step $t$.\n",
    "      - $\\mathbf{W}$: Combined weight matrix\n",
    "      - $\\mathbf{b}$: Combined bias vector\n",
    "      - $\\mathbf{W}_ho$: Weight matrix for hidden to output\n",
    "      - $\\mathbf{b}_ho$: Bias for hidden to output\n",
    "      - $\\mathbf{\\sigma}$: Activation function (e.g., Tanh, Sigmoid, ReLU)\n",
    "      - $\\mathbf{g}$: Activation function for output (e.g., Softmax for classification)\n",
    "   - **Formulations**:\n",
    "      - Concatenation of Input and Hidden State:\n",
    "      $$\\mathbf{z}_t = [\\mathbf{x}_t; \\mathbf{h}_{t-1}]$$\n",
    "      - Hidden State Calculation:\n",
    "      $$\\mathbf{h}_t = \\sigma(\\mathbf{W} \\mathbf{z}_t + \\mathbf{b})$$\n",
    "      - Output Calculation:\n",
    "      $$\\mathbf{y}_t = g(\\mathbf{W}_{ho} \\mathbf{h}_t + \\mathbf{b}_{ho})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # combined weight matrix for input to hidden and hidden to hidden\n",
    "        self.W = nn.Parameter(torch.randn(hidden_dim, input_dim + hidden_dim))\n",
    "        self.b = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "        # weights for hidden to output connection\n",
    "        self.W_ho = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
    "        self.b_ho = nn.Parameter(torch.randn(output_dim))\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        combined = torch.cat((input, hidden), dim=1)  # concatenate input and hidden state\n",
    "        hidden = torch.tanh(combined @ self.W.T + self.b)\n",
    "        output = hidden @ self.W_ho.T + self.b_ho\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        # initialize the hidden state with zeros (h_0)\n",
    "        return torch.zeros(batch_size, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 5\n",
    "\n",
    "# Create RNN\n",
    "rnn_2 = VanillaRNN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()               : torch.Size([128, 5, 10])\n",
      "y.size()               : torch.Size([128])\n",
      "x.size() [first batch] : torch.Size([32, 5, 10])\n",
      "y.size() [first batch] : torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Example input (num data, sequence length, input dim)\n",
    "num_data = 128\n",
    "sequence_length = 5\n",
    "x = torch.randn(num_data, sequence_length, input_dim)\n",
    "y = torch.randn(num_data)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(x, y)\n",
    "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Log\n",
    "print(f\"x.size()               : {x.size()}\")\n",
    "print(f\"y.size()               : {y.size()}\")\n",
    "print(f\"x.size() [first batch] : {next(iter(trainsetloader))[0].size()}\")\n",
    "print(f\"y.size() [first batch] : {next(iter(trainsetloader))[1].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 1/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 2/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 3/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
      "batch: 4/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the RNN\n",
    "for c, (x, y_true) in enumerate(trainsetloader):\n",
    "    # Initialize hidden state\n",
    "    hidden = rnn_2.init_hidden(batch_size)\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        y_pred, hidden = rnn_2(x[:, i, :], hidden)\n",
    "        print(f\"batch: {c+1}/{len(trainsetloader)} | time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VanillaRNN                               [32, 5]                   725\n",
       "==========================================================================================\n",
       "Total params: 725\n",
       "Trainable params: 725\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(rnn_2, input_size=((batch_size, input_dim), hidden.size()), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/recurrent-neural-networks-3.svg\" alt=\"recurrent-neural-networks-3.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Deep Recurrent Neural Networks</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 1/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 1/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 1/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 1/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 2/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 2/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 2/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 2/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 2/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 3/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 3/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 3/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 3/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 3/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 4/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 4/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 4/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 4/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
      "Batch: 4/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "class DeepVanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int) -> None:\n",
    "        super(DeepVanillaRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define RNN layers\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.rnn_layers.append(nn.Linear(input_dim + hidden_dim, hidden_dim))\n",
    "            else:\n",
    "                self.rnn_layers.append(nn.Linear(hidden_dim + hidden_dim, hidden_dim))\n",
    "                \n",
    "        # define the output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        combined_input = torch.cat((input, hidden[0]), dim=1)  # concatenate input and the first hidden state along the feature dimension\n",
    "        new_hidden = []\n",
    "\n",
    "        for i, rnn_layer in enumerate(self.rnn_layers):\n",
    "            hidden_state = torch.tanh(rnn_layer(combined_input))\n",
    "            new_hidden.append(hidden_state)\n",
    "            combined_input = torch.cat((hidden_state, hidden[i]), dim=1)  # concatenate the current hidden state with the previous one\n",
    "\n",
    "        # use the last hidden state for output\n",
    "        final_hidden = new_hidden[-1]\n",
    "        output = self.output_layer(final_hidden)\n",
    "        return output, torch.stack(new_hidden)\n",
    "\n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        # initialize hidden state with zeros for each layer and batch\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "# parameters\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 5\n",
    "num_layers = 3\n",
    "\n",
    "# create Deep RNN\n",
    "deep_rnn = DeepVanillaRNN(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# example input\n",
    "num_data = 128\n",
    "sequence_length = 5\n",
    "x = torch.randn(num_data, sequence_length, input_dim)\n",
    "y = torch.randn(num_data)\n",
    "\n",
    "# create dataset and dataloader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(x, y)\n",
    "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# training loop\n",
    "for c, (x, y_true) in enumerate(trainsetloader):\n",
    "    # initialize hidden state for each batch\n",
    "    hidden = deep_rnn.init_hidden(batch_size)\n",
    "    \n",
    "    for i in range(sequence_length):\n",
    "        y_pred, hidden = deep_rnn(x[:, i, :], hidden)\n",
    "        print(f\"Batch: {c+1}/{len(trainsetloader)} | Time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepVanillaRNN(\n",
       "  (rnn_layers): ModuleList(\n",
       "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (1-2): 2 x Linear(in_features=40, out_features=20, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=20, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DeepVanillaRNN                           [32, 5]                   --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Linear: 2-1                       [32, 20]                  620\n",
       "│    └─Linear: 2-2                       [32, 20]                  820\n",
       "│    └─Linear: 2-3                       [32, 20]                  820\n",
       "├─Linear: 1-2                            [32, 5]                   105\n",
       "==========================================================================================\n",
       "Total params: 2,365\n",
       "Trainable params: 2,365\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.08\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(deep_rnn, input_size=((batch_size, input_dim), hidden.size()), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "   - A type of recurrent neural network (RNN) aimed at dealing with the vanishing gradient problem present in traditional RNNs.\n",
    "   - It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".\n",
    "   - It is based on the [Long Short-term Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) paper, Developed in 1997 by [Sepp Hochreiter](https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en) and [Jürgen Schmidhuber](https://scholar.google.com/citations?user=gLnCTgIAAAAJ&hl=en).\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/recurrent-neural-networks-4.svg\" alt=\"recurrent-neural-networks-4.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Long Short-Term Memory (LSTM)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRU)\n",
    "   - A gating mechanism in recurrent neural networks, introduced in 2014 by [Kyunghyun](https://dblp.uni-trier.de/search/author?author=Kyunghyun%20Cho).\n",
    "   - It is based on the [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) paper.\n",
    "   - Similar to LSTM but lacks a `context vector` or `output gate`, resulting in fewer parameters than LSTM.\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../resources/images/SVGs/recurrent-neural-networks-5.svg\" alt=\"recurrent-neural-networks-5.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Gated Recurrent Units (GRU)</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

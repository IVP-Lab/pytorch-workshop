{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a fixed seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error(MSE) loss function\n",
    "   - ### $L(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$\n",
    "\n",
    "## Binary Cross-Entropy(BCE) loss function\n",
    "   - ### $L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    return (y_pred - y_true) ** 2\n",
    "\n",
    "def mean_squared_error_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    return np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss_per_sample(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    # to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "\n",
    "    # clip predicted values to avoid taking the log of 0 or 1\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    # to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "\n",
    "    # clip predicted values to avoid taking the log of 0 or 1\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: [0 0 0]\n",
      "y_pred: [0.5       0.7500052 0.9999546]\n"
     ]
    }
   ],
   "source": [
    "# we have 3 samples for a binary classification\n",
    "y_true = np.array([[0], [0], [0]])\n",
    "\n",
    "# output of model_1\n",
    "output = np.array([[0], [1.09864], [10]])\n",
    "y_pred = sigmoid(output)\n",
    "\n",
    "# log\n",
    "print(f\"y_true: {y_true.squeeze()}\")\n",
    "print(f\"y_pred: {y_pred.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELoss  [per sample]: [0.25       0.56250779 0.99990921]\n",
      "BCELoss [per sample]: [ 0.69314718  1.38631514 10.0000454 ]\n",
      "MSELoss             : 0.60414\n",
      "BCELoss             : 4.02650\n"
     ]
    }
   ],
   "source": [
    "loss_1 = squared_error_loss(y_pred, y_true).squeeze()\n",
    "loss_2 = binary_cross_entropy_loss_per_sample(y_pred, y_true).squeeze()\n",
    "loss_3 = mean_squared_error_loss(y_pred, y_true)\n",
    "loss_4 = binary_cross_entropy_loss(y_pred, y_true)\n",
    "\n",
    "# log\n",
    "print(f\"SELoss  [per sample]: {loss_1}\")\n",
    "print(f\"BCELoss [per sample]: {loss_2}\")\n",
    "print(f\"MSELoss             : {loss_3:.5f}\")\n",
    "print(f\"BCELoss             : {loss_4:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss & BCELoss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "y_true = np.zeros(shape= (100, 1))\n",
    "y_pred = sigmoid(np.linspace(-10, +10, 100).reshape(-1, 1))\n",
    "bce_loss = binary_cross_entropy_loss_per_sample(y_pred, y_true)\n",
    "mse_loss = squared_error_loss(y_pred, y_true)\n",
    "\n",
    "plt.plot(y_pred, bce_loss, label= 'BCELoss')\n",
    "plt.plot(y_pred, mse_loss, label= 'MSELoss')\n",
    "plt.title(f\"y_true: {y_true[0, 0]}\")\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 10, 2\n",
    "\n",
    "x, y = datasets.make_classification(n_samples= n_samples, n_features= n_features, n_informative= 2, n_redundant= 0, n_clusters_per_class= 1, random_state= 42)\n",
    "\n",
    "plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1], color= 'b', label= 'Class 0')\n",
    "plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1], color= 'r', label= 'Class 1')\n",
    "plt.title('Generated Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy.ndarray to torch.Tensor\n",
    "train_x = torch.from_numpy(x.astype(np.float32))\n",
    "train_y = torch.from_numpy(y.astype(np.float32)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (classifier): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a custom logistic regression model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.classifier = torch.nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = LogisticRegression(n_features, 1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom BCELoss() function\n",
    "class BCELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.nn.BCELoss()(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 -> loss: 0.38442\n",
      "epoch: 1 -> loss: 0.34141\n",
      "epoch: 2 -> loss: 0.30671\n",
      "epoch: 3 -> loss: 0.27825\n",
      "epoch: 4 -> loss: 0.25456\n",
      "epoch: 5 -> loss: 0.23458\n"
     ]
    }
   ],
   "source": [
    "state = []\n",
    "\n",
    "# initial weight\n",
    "with torch.no_grad():\n",
    "    model.classifier.weight[0, 0].fill_(-1)\n",
    "    model.classifier.weight[0, 1].fill_(1)\n",
    "    model.classifier.bias[0].fill_(1)\n",
    "\n",
    "# hyper parameters\n",
    "epoch = 6\n",
    "lr = .5\n",
    "criterion = BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= lr)\n",
    "\n",
    "# training loop\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "    \n",
    "    # forward\n",
    "    y_pred = model(train_x)\n",
    "\n",
    "    # backward\n",
    "    loss = criterion(y_pred, train_y)\n",
    "    loss.backward()\n",
    "\n",
    "    # save new y_pred every 5 epochs\n",
    "    state.append([model.classifier.weight.detach().clone().numpy(), model.classifier.bias.detach().clone().numpy()])\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # log\n",
    "    print(f\"epoch: {i} -> loss: {loss.item():>7.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows= 3, ncols= 2, figsize= (12, 16), layout= 'compressed')\n",
    "\n",
    "for row in range(3):\n",
    "    for col in range(2):\n",
    "        axs[row, col].scatter(x[y == 0][:, 0], x[y == 0][:, 1], color= 'b', label= 'Class 0')\n",
    "        axs[row, col].scatter(x[y == 1][:, 0], x[y == 1][:, 1], color= 'r', label= 'Class 1')\n",
    "        axs[row, col].set(title= f\"epoch {row * 2 + col}, W: {state[row * 2 + col][0].squeeze()}, b: {state[row * 2 + col][1].squeeze():.3f}\", xlim= (x[:, 0].min() - 1, x[:, 0].max() + 1), ylim= (x[:, 1].min() - 1, x[:, 1].max() + 1))\n",
    "\n",
    "        # decision boundary\n",
    "        w, b = state[row * 2 + col]\n",
    "        slope = -w[0][0] / w[0][1]\n",
    "        intercept = -b[0] / w[0][1]\n",
    "        x_plot = np.array([np.min(x[:, 0]), np.max(x[:, 0])])\n",
    "        y_plot = slope * x_plot + intercept\n",
    "\n",
    "        axs[row, col].plot(x_plot, y_plot, color='g', linestyle='--', label='Decision Boundary')\n",
    "        axs[row, col].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
